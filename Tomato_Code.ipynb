{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 544347,
          "sourceType": "datasetVersion",
          "datasetId": 259770
        }
      ],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Tomato_Code",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gblack98/PestVisionAI/blob/Gabar_001/Tomato_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "noulam_tomato_path = kagglehub.dataset_download('noulam/tomato')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "X6c1eZi-rrZE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import cv2  # Pour la lecture d'images si besoin (optionnel)\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Configuration centrale des hyperparamètres\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "NUM_CLASSES = 10\n",
        "SEED = 1337\n",
        "\n",
        "# Reproductibilité\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# Fonction de création du modèle amélioré\n",
        "def create_model(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    model = Sequential([\n",
        "        # Bloc convolutif 1\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Bloc convolutif 2\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.35),\n",
        "\n",
        "        # Bloc convolutif 3\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.45),\n",
        "\n",
        "        # Partie entièrement connectée\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Préparation des données avec augmentation avancée\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.7, 1.3],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Chargement des données\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/kaggle/input/tomato/New Plant Diseases Dataset(Augmented)/train',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/kaggle/input/tomato/New Plant Diseases Dataset(Augmented)/valid',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Création et compilation du modèle\n",
        "model = create_model()\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks améliorés\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=1e-7),\n",
        "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Entraînement\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T17:51:10.983633Z",
          "iopub.execute_input": "2025-02-08T17:51:10.983964Z"
        },
        "id": "KMz_4xkQrrZG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation finale\n",
        "final_loss, final_acc = model.evaluate(val_generator)\n",
        "print(f'\\nValidation Accuracy: {final_acc:.2%}')\n",
        "print(f'Validation Loss: {final_loss:.4f}')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HZk0zNvYrrZH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Visualisations avancées\n",
        "# -------------------------------\n",
        "\n",
        "# 1. Courbes d'entraînement (Loss et Accuracy)\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Loss entraînement')\n",
        "plt.plot(history.history['val_loss'], label='Loss validation')\n",
        "plt.title('Courbes de Loss')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy entraînement')\n",
        "plt.plot(history.history['val_accuracy'], label='Accuracy validation')\n",
        "plt.title('Courbes d\\'Accuracy')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OvXJY1PurrZH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Matrice de confusion et rapport de classification\n",
        "# Obtenir les prédictions sur l'ensemble de validation\n",
        "val_generator.reset()\n",
        "Y_pred = model.predict(val_generator, steps=val_generator.samples // BATCH_SIZE + 1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "class_labels = list(val_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Matrice de Confusion')\n",
        "plt.xlabel('Prédiction')\n",
        "plt.ylabel('Vérité')\n",
        "plt.show()\n",
        "\n",
        "# Rapport de classification\n",
        "print(\"Rapport de Classification :\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels))"
      ],
      "metadata": {
        "trusted": true,
        "id": "XfCi1F8errZI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Affichage d'exemples d'images avec leurs prédictions\n",
        "# Récupérer un batch d'images de validation\n",
        "val_batch = next(val_generator)\n",
        "images, labels = val_batch[0], val_batch[1]\n",
        "predictions = model.predict(images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(labels, axis=1)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "for i in range(12):\n",
        "    idx = random.randint(0, images.shape[0]-1)\n",
        "    plt.subplot(3,4,i+1)\n",
        "    plt.imshow(images[idx])\n",
        "    plt.title(f\"Vrai: {class_labels[true_labels[idx]]}\\nPréd: {class_labels[predicted_labels[idx]]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "X3KY22nRrrZL"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}